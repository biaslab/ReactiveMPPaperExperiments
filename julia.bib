@misc{quadgk,
 howpublished  = {\url{https://github.com/JuliaMath/QuadGK.jl}},
 author        = {Johnson, Steven  G.},
 title         = {{QuadGK.jl}: {G}auss--{K}ronrod integration in {J}ulia}
}

@article{DifferentialEquations.jl-2017,
 author        = {Rackauckas, Christopher and Nie, Qing},
 journal       = {The Journal of Open Research Software},
 title         = {DifferentialEquations.jl – A Performant and Feature-Rich Ecosystem for Solving Differential Equations in Julia},
 number        = {1},
 doi           = {10.5334/jors.151},
 keywords      = {Applied Mathematics},
 note          = {Exported from https://app.dimensions.ai on 2019/05/05},
 year          = {2017},
 url           = {https://app.dimensions.ai/details/publication/pub.1085583166 and http://openresearchsoftware.metajnl.com/articles/10.5334/jors.151/galley/245/download/},
 volume        = {5}
}

@article{Julia-2017,
 pages         = {65--98},
 author        = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral  B},
 journal       = {SIAM {R}eview},
 title         = {Julia: A fresh approach to numerical computing},
 publisher     = {SIAM},
 number        = {1},
 doi           = {10.1137/141000671},
 year          = {2017},
 url           = {https://epubs.siam.org/doi/10.1137/141000671},
 volume        = {59}
}

@inproceedings{ge2018t,
 biburl        = {https://dblp.org/rec/bib/conf/aistats/GeXG18},
 pages         = {1682--1690},
 booktitle     = {International Conference on Artificial Intelligence and Statistics,
               {AISTATS} 2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary Islands,
               Spain},
 author        = {Ge, Hong and Xu, Kai and Ghahramani, Zoubin},
 year          = {2018},
 url           = {http://proceedings.mlr.press/v84/ge18b.html},
 title         = {Turing: a language for flexible probabilistic inference}
}

@misc{BenchmarkTools.jl-2016,
 author        = {{Chen}, Jiahao and {Revels}, Jarrett},
 adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
 month         = {Aug},
 journal       = {arXiv e-prints},
 title         = {{Robust benchmarking in noisy environments}},
 eid           = {arXiv:1608.04295},
 keywords      = {Computer Science - Performance, 68N30, B.8.1, D.2.5},
 primaryclass  = {cs.PF},
 archiveprefix = {arXiv},
 eprint        = {1608.04295},
 year          = {2016},
 url           = {https://arxiv.org/abs/1608.04295},
 adsurl        = {https://ui.adsabs.harvard.edu/abs/2016arXiv160804295C}
}

@article{ForneyLab.jl-2019,
 pages         = {185--204},
 author        = {Cox, Marco and  van de Laar, Thijs and  de Vries, Bert},
 month         = {January},
 journal       = {International Journal of Approximate Reasoning},
 title         = {A factor graph approach to automated design of {Bayesian} signal processing algorithms},
 doi           = {10.1016/j.ijar.2018.11.002},
 keywords      = {Bayesian inference, Message passing, Factor graphs, Julia, Probabilistic programming},
 issn          = {0888-613X},
 urldate       = {2018-11-16},
 year          = {2019},
 url           = {http://www.sciencedirect.com/science/article/pii/S0888613X18304298},
 volume        = {104},
 abstract      = {The benefits of automating design cycles for Bayesian inference-based algorithms are becoming increasingly recognized by the machine learning community. As a result, interest in probabilistic programming frameworks has much increased over the past few years. This paper explores a specific probabilistic programming paradigm, namely message passing in Forney-style factor graphs (FFGs), in the context of automated design of efficient Bayesian signal processing algorithms. To this end, we developed “ForneyLab”2 as a Julia toolbox for message passing-based inference in FFGs. We show by example how ForneyLab enables automatic derivation of Bayesian signal processing algorithms, including algorithms for parameter estimation and model comparison. Crucially, due to the modular makeup of the FFG framework, both the model specification and inference methods are readily extensible in ForneyLab. In order to test this framework, we compared variational message passing as implemented by ForneyLab with automatic differentiation variational inference (ADVI) and Monte Carlo methods as implemented by state-of-the-art tools “Edward” and “Stan”. In terms of performance, extensibility and stability issues, ForneyLab appears to enjoy an edge relative to its competitors for automated inference in state-space models.}
}

@article{Optim.jl-2018,
 number        = {24},
 pages         = {615},
 doi           = {10.21105/joss.00615},
 author        = {Mogensen, Patrick  Kofod and Riseth, Asbj{\o}rn  Nilsen},
 year          = {2018},
 volume        = {3},
 journal       = {Journal of Open Source Software},
 title         = {Optim: A mathematical optimization package for {Julia}}
}

@article{FFTW.jl-2005,
 number        = {2},
 pages         = {216--231},
 doi           = {10.1109/JPROC.2004.840301},
 author        = {Frigo, Matteo and Johnson, Steven~G.},
 note          = {Special issue on ``Program Generation, Optimization, and Platform Adaptation''},
 year          = {2005},
 volume        = {93},
 journal       = {Proceedings of the IEEE},
 title         = {The Design and Implementation of {FFTW3}}
}
