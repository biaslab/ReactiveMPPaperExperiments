{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "using DrWatson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@quickactivate :ReactiveMPPaperExperiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForneyLab\n",
    "using BenchmarkTools\n",
    "using Random\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we want to compare results and performance of ReactiveMP.jl with another probabilistic programming library which is called ForneyLab.jl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "d = 2\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function make_benchmark_data(n, d, seed)\n",
    "    rng = MersenneTwister(seed)\n",
    "\n",
    "    A = random_rotation_matrix(rng, d)\n",
    "    B = Matrix(Diagonal(ones(d) .+ rand(rng, -0.5:0.1:1.0, d)))\n",
    "    P = Matrix(Diagonal(2.0 * ones(d)))\n",
    "    Q = Matrix(Diagonal(2.0 * ones(d)))\n",
    "\n",
    "    params = @strdict n d seed A B P Q\n",
    "\n",
    "    x, y = generate_data(LGSSMModel(), params)\n",
    "\n",
    "    return x, y, params\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data, params = make_benchmark_data(n, d, seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = FactorGraph()\n",
    "\n",
    "model_d = params[\"d\"]\n",
    "model_n = params[\"n\"]\n",
    "model_A = params[\"A\"]\n",
    "model_B = params[\"B\"]\n",
    "model_P = params[\"P\"]\n",
    "model_Q = params[\"Q\"]\n",
    "\n",
    "@RV x0 ~ GaussianMeanVariance(zeros(model_d), Matrix(Diagonal(100.0 * ones(model_d)))) # Prior\n",
    "\n",
    "x = Vector{Variable}(undef, model_n) # Pre-define vectors for storing latent and observed variables\n",
    "y = Vector{Variable}(undef, model_n)\n",
    "\n",
    "x_t_prev = x0\n",
    "\n",
    "for t = 1:model_n\n",
    "    @RV x[t] ~ GaussianMeanVariance(model_A*x_t_prev, model_P) # Process model\n",
    "    @RV y[t] ~ GaussianMeanVariance(model_B*x[t], model_Q) # Observation model\n",
    "\n",
    "    placeholder(y[t], :y, dims=(model_d,), index=t) # Indicate observed variable\n",
    "    \n",
    "    x_t_prev = x[t] # Prepare state for next section\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First invocation triggers the compilation\n",
    "@time begin\n",
    "    algo = messagePassingAlgorithm(x) # Generate algorithm\n",
    "    source_code = algorithmSourceCode(algo) # Generate source code for algorithm\n",
    "    eval(Meta.parse(source_code)); # Load algorithm\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time begin\n",
    "    algo = messagePassingAlgorithm(x) # Generate algorithm\n",
    "    source_code = algorithmSourceCode(algo) # Generate source code for algorithm\n",
    "    eval(Meta.parse(source_code)); # Load algorithm\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DIMENSION = 2\n",
    "# NOTE: ForneyLab compilation times vary from time to time\n",
    "# Because of the global evaluation we cannot robustly perform this benchmark in a loop like we did for Turing and ReactiveMP\n",
    "# So we change number of observations by hand and perform benchmarks separately \n",
    "# ForneyLab model creation time times\n",
    "# 100 - 47.139422 seconds (5.06 M allocations: 2.732 GiB, 1.20% gc time)\n",
    "# 200 - 86.241535 seconds (10.19 M allocations: 5.557 GiB, 1.28% gc time)\n",
    "# 250 - 117.509299 seconds (12.82 M allocations: 7.001 GiB, 1.32% gc time)\n",
    "# 300 - 137.006022 seconds (15.31 M allocations: 8.469 GiB, 1.22% gc time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DIMENSION = 4\n",
    "# NOTE: ForneyLab compilation times vary from time to time\n",
    "# Because of the global evaluation we cannot robustly perform this benchmark in a loop like we did for Turing and ReactiveMP\n",
    "# So we change number of observations by hand and perform benchmarks separately \n",
    "# ForneyLab model creation time times\n",
    "# 100 - 46.829628 seconds (5.27 M allocations: 2.771 GiB, 1.31% gc time)\n",
    "# 200 - 88.016589 seconds (10.25 M allocations: 5.661 GiB, 1.20% gc time)\n",
    "# 250 - 119.371944 seconds (13.33 M allocations: 7.185 GiB, 1.83% gc time)\n",
    "# 300 - 136.036903 seconds (15.92 M allocations: 8.722 GiB, 1.19% gc time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dict(:y => y_data) # Prepare data dictionary\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First invocation triggers the compilation\n",
    "@time step!(data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DIMENSION = 2\n",
    "# Note: Interesting observation, the compilation times depends on the hardware, but usually at the same order of magnitude\n",
    "# ForneyLab compilation time\n",
    "# 100 - 65.614497 seconds (3.17 M allocations: 139.568 MiB, 0.09% gc time, 99.94% compilation time)\n",
    "# 200 - 288.590772 seconds (6.21 M allocations: 269.570 MiB, 0.03% gc time, 99.99% compilation time)\n",
    "# 250 - 557.830418 seconds (7.73 M allocations: 332.040 MiB, 0.02% gc time, 99.94% compilation time)\n",
    "# 300 - 920.685297 seconds (9.25 M allocations: 395.463 MiB, 0.01% gc time, 99.97% compilation time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DIMENSION = 4\n",
    "# Note: Interesting observation, the compilation times depends on the hardware, but usually at the same order of magnitude\n",
    "# Note: The compilation time does not depend on the DIMENSION (which is expected, it should not)\n",
    "# ForneyLab compilation time\n",
    "# 100 - 65.860935 seconds (3.27 M allocations: 143.514 MiB, 0.06% gc time, 99.92% compilation time)\n",
    "# 200 - 318.073160 seconds (6.42 M allocations: 277.403 MiB, 0.03% gc time, 100.00% compilation time)\n",
    "# 250 - 569.752148 seconds (8.05 M allocations: 345.758 MiB, 0.04% gc time, 99.95% compilation time) \n",
    "# 300 - 868.833146 seconds (9.56 M allocations: 407.149 MiB, 0.01% gc time, 99.99% compilation time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `@btime` does not include the compilation and performs the benchmark multiple times\n",
    "@btime step!(data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred = step!(data) # Execute inference\n",
    "marginals = map(i -> inferred[Symbol(:x_, i)], 1:model_n) \n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mse(x_data, marginals) ./ n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DIMENSION = 2\n",
    "# Note: E[MSE] scaled by `n` and is identical to the RMP case (see post-benchmark table in the RMP notebook)\n",
    "# Both ForneyLab and RMP use mesage-passing and their identical results are expected\n",
    "# ForneyLab execution times and E[MSE]\n",
    "# 100 - 4.036 ms (39642 allocations: 4.15 MiB) | E[MSE] = 4.1718819514395955\n",
    "# 200 - 6.128 ms (79695 allocations: 8.34 MiB) | E[MSE] = 4.082831166403034\n",
    "# 250 - 10.248 ms (99600 allocations: 10.42 MiB) | E[MSE] = 4.0585925696062155\n",
    "# 300 - 11.570 ms (119501 allocations: 12.50 MiB) | E[MSE] = 3.96470532342949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DIMENSION = 4\n",
    "# Note: E[MSE] scaled by `n` and is identical to the RMP case (see post-benchmark table in the RMP notebook)\n",
    "# Both ForneyLab and RMP use mesage-passing and their identical results are expected\n",
    "# ForneyLab execution times and E[MSE]\n",
    "# 100 - 3.579 ms (39642 allocations: 4.60 MiB) | E[MSE] = 8.004955278503026\n",
    "# 200 - 7.681 ms (79695 allocations: 9.24 MiB) | E[MSE] = 7.6515537436790035\n",
    "# 250 - 12.099 ms (99600 allocations: 11.55 MiB) | E[MSE] = 7.736329375309611\n",
    "# 300 - 12.685 ms (119501 allocations: 13.86 MiB) | E[MSE] = 7.750883079446451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CairoMakie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this graph is not the same as Figure 11 in the paper. The figure 11 uses a diferent (hand-crafted) dataset\n",
    "# while the benchmarks use the `make_benchmark_data` function (which produces identical data across all benchmarks)\n",
    "let\n",
    "    c = Makie.wong_colors() \n",
    "\n",
    "    x_inferred_means = ForneyLab.unsafeMean.(marginals)\n",
    "    x_inferred_stds  = map(e -> sqrt.(e), ForneyLab.unsafeVar.(marginals))\n",
    "    \n",
    "    range = 1:length(x_inferred_means)\n",
    "\n",
    "    fig = Figure(resolution = (550, 350))\n",
    "    ax  = Axis(fig[1, 1], xlabel = \"Time step k\", ylabel = \"Latent states\")\n",
    "\n",
    "    # Real dim1\n",
    "    lines!(ax, range, x_data |> edim(1), color = :red3, linewidth = 1.75, label = \"x[:, 1]\",)\n",
    "    scatter!(ax, range, y_data |> edim(1), color = (:red3, 0.35), markersize = 10, marker = :cross)\n",
    "\n",
    "    # Estimated dim1\n",
    "\n",
    "    lines!(ax, range, x_inferred_means |> edim(1), color = c[3], label = \"estimated[:, 1]\")\n",
    "    band!(ax, range, (x_inferred_means |> edim(1)) .+ (x_inferred_stds |> edim(1)), (x_inferred_means |> edim(1)) .- (x_inferred_stds |> edim(1)), color = (c[3], 0.65))\n",
    "\n",
    "    # Real dim2\n",
    "\n",
    "    lines!(ax, range, x_data |> edim(2), color = :purple, linewidth = 1.75, linestyle = :dash, label = \"x[:, 2]\")\n",
    "    scatter!(ax, range, y_data |> edim(2), color = (:purple, 0.35),markersize = 6, marker = :circle)\n",
    "\n",
    "    # Estimated dim2\n",
    "\n",
    "    lines!(ax, range, x_inferred_means |> edim(2), color = c[1], label = \"estimated[:, 2]\")\n",
    "    band!(ax, range, (x_inferred_means |> edim(2)) .+ (x_inferred_stds |> edim(2)), (x_inferred_means |> edim(2)) .- (x_inferred_stds |> edim(2)), color = (c[1], 0.65))\n",
    "\n",
    "    axislegend(ax, position = :lt)\n",
    "\n",
    "    fig\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.7",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
